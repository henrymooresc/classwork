{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='heading'>\n",
    "    <div style='float:left;'><h1>CPSC 4300/6300: Applied Data Science</h1></div>\n",
    "    <img style=\"float: right; padding-right: 10px; width: 65px\" src=\"https://bsethwalker.github.io/assets/img/clemson_paw.png\"> </div>\n",
    "\n",
    "## Week 4: Regressions\n",
    "\n",
    "**Clemson University**<br>\n",
    "**Fall 2021**<br>\n",
    "**Instructor(s):** Nina Hubig <br>\n",
    "**Author(s):** Chris Kalahiki, Brandon Walker\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "\n",
       "div.heading {\n",
       "margin-bottom: 25px;\n",
       "height: 75px;\n",
       "}\n",
       "\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "    \n",
       "    background: rgba(245, 102, 0, .75);\n",
       "    border-color: #E9967A;\n",
       "    border-left: 5px solid #522D80; \n",
       "    padding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "    background-color: #fce8e8;\n",
       "    border-color: #E9967A; \t\n",
       "    border-left: 5px solid #800080; \n",
       "    padding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "    font-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "    background-color: #DDDDDD;\n",
       "    border-color: #E9967A; \t\n",
       "    border-left: 5px solid #800080; \n",
       "    padding: 0.5em;\n",
       "    font-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "    background-color: #AEDE94;\n",
       "    border-color: #E9967A; \t \n",
       "    border-left: 5px solid #800080; \n",
       "    padding: 0.5em;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://bsethwalker.github.io/assets/css/cpsc6300.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "* Feel comfortable with simple linear regression\n",
    "* Feel comfortable with polynomial regression\n",
    "* Feel comfortable with multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scatter_matrix\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msm\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OLS\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "## 1 - Simple Linear Regression\n",
    "\n",
    "Linear regression and its many extensions are a workhorse of the statistics and data science community, both in application and as a reference point for other models. Most of the major concepts in machine learning can be and often are discussed in terms of various linear regression models. Thus, this section will introduce you to building and fitting linear regression models and some of the process behind it, so that you can 1) fit models to data you encounter 2) experiment with different kinds of linear regression and observe their effects 3) see some of the technology that makes regression models work.\n",
    "\n",
    "\n",
    "### Linear regression with a toy dataset\n",
    "We first examine a toy problem, focusing our efforts on fitting a linear model to a small dataset with three observations.  Each observation consists of one predictor $x_i$ and one response $y_i$ for $i = 1, 2, 3$,\n",
    "\n",
    "\\begin{align*}\n",
    "(x , y) = \\{(x_1, y_1), (x_2, y_2), (x_3, y_3)\\}.\n",
    "\\end{align*}\n",
    "\n",
    "To be very concrete, let's set the values of the predictors and responses.\n",
    "\n",
    "\\begin{equation*}\n",
    "(x , y) = \\{(1, 2), (2, 2), (3, 4)\\}\n",
    "\\end{equation*}\n",
    "\n",
    "There is no line of the form $\\beta_0 + \\beta_1 x = y$ that passes through all three observations, since the data are not collinear. Thus our aim is to find the line that best fits these observations in the *least-squares sense*, as discussed in lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "* Make two numpy arrays out of this data, x_train and y_train\n",
    "* Check the dimentions of these arrays\n",
    "* Try to reshape them into a different shape\n",
    "* Make points into a very simple scatterplot\n",
    "* Make a better scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "x_train = np.array([1,2,3])\n",
    "y_train = np.array([2,3,6])\n",
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(1,3)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/simple_scatterplot.py\n",
    "# Make a simple scatterplot\n",
    "x_train = np.array([1,2,3])\n",
    "y_train = np.array([2,2,4])\n",
    "plt.scatter(x_train,y_train)\n",
    "\n",
    "# check dimensions \n",
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/nice_scatterplot.py\n",
    "def nice_scatterplot(x, y, title):\n",
    "    # font size\n",
    "    f_size = 18\n",
    "    \n",
    "    # make the figure\n",
    "    fig, ax = plt.subplots(1,1, figsize=(8,5)) # Create figure object\n",
    "\n",
    "    # set axes limits to make the scale nice\n",
    "    ax.set_xlim(np.min(x)-1, np.max(x) + 1)\n",
    "    ax.set_ylim(np.min(y)-1, np.max(y) + 1)\n",
    "\n",
    "    # adjust size of tickmarks in axes\n",
    "    ax.tick_params(labelsize = f_size)\n",
    "    \n",
    "    # remove tick labels\n",
    "    #ax.tick_params(labelbottom=False, bottom=False)\n",
    "    \n",
    "    # adjust size of axis label\n",
    "    ax.set_xlabel(r'$x$', fontsize = f_size)\n",
    "    ax.set_ylabel(r'$y$', fontsize = f_size)\n",
    "    \n",
    "    # set figure title label\n",
    "    ax.set_title(title, fontsize = f_size)\n",
    "\n",
    "    # you may set up grid with this \n",
    "    ax.grid(True, lw=1.75, ls='--', alpha=0.15)\n",
    "\n",
    "    # make actual plot (Notice the label argument!)\n",
    "    ax.scatter(x, y, label=r'$my points$')\n",
    "    ax.legend(loc='best', fontsize = f_size)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "nice_scatterplot(x_train, y_train, 'hello nice plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Formulae\n",
    "Linear regression is special among the models we study because it can be solved explicitly. While most other models (and even some advanced versions of linear regression) must be solved itteratively, linear regression has a formula where you can simply plug in the data.\n",
    "\n",
    "For the single predictor case it is:\n",
    "$\\beta_1 = \\frac{\\sum_{i=1}^n{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sum_{i=1^n{(x_i-\\bar{x})^2}}\\beta_0 = \\bar{y} - \\beta_1\\bar{x}}$\n",
    "    \n",
    "Where $\\bar{y}$ and $\\bar{x}$ are the mean of the y values and the mean of the x values, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Building a model from scratch\n",
    "In this part, we will solve the equations for simple linear regression and find the best fit solution to our toy problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snippets of code below implement the linear regression equations on the observed predictors and responses, which we'll call the training data set.  Let's walk through the code.\n",
    "\n",
    "We have to reshape our arrrays to 2D. We will see later why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "* make an array with shape (2,3)\n",
    "* reshape it to a size that you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution\n",
    "xx = np.array([[1,2,3],[4,6,8]])\n",
    "xxx = xx.reshape(-1,2)\n",
    "xxx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to be a proper 2D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 1)\n",
    "y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, compute means\n",
    "y_bar = np.mean(y_train)\n",
    "x_bar = np.mean(x_train)\n",
    "\n",
    "# build the two terms\n",
    "numerator = np.sum( (x_train - x_bar)*(y_train - y_bar) )\n",
    "denominator = np.sum((x_train - x_bar)**2)\n",
    "\n",
    "print(numerator.shape, denominator.shape) #check shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why the empty brackets? (The numerator and denominator are scalars, as expected.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slope beta1\n",
    "beta_1 = numerator/denominator\n",
    "\n",
    "#intercept beta0\n",
    "beta_0 = y_bar - beta_1*x_bar\n",
    "\n",
    "print(\"The best-fit line is {0:3.2f} + {1:3.2f} * x\".format(beta_0, beta_1))\n",
    "print(f'The best fit is {beta_0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "Turn the code from the above cells into a function called `simple_linear_regression_fit`, that inputs the training data and returns `beta0` and `beta1`.\n",
    "\n",
    "To do this, copy and paste the code from the above cells below and adjust the code as needed, so that the training data becomes the input and the betas become the output.\n",
    "\n",
    "```python\n",
    "def simple_linear_regression_fit(x_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    return\n",
    "```\n",
    "\n",
    "Check your function by calling it with the training data from above and printing out the beta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/simple_linear_regression_fit.py\n",
    "def simple_linear_regression_fit(x_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    x_train: a (num observations by 1) array holding the values of the predictor variable\n",
    "    y_train: a (num observations by 1) array holding the values of the response variable\n",
    "\n",
    "    Returns:\n",
    "    beta_vals:  a (num_features by 1) array holding the intercept and slope coeficients\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check input array sizes\n",
    "    if len(x_train.shape) < 2:\n",
    "        print(\"Reshaping features array.\")\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1)\n",
    "\n",
    "    if len(y_train.shape) < 2:\n",
    "        print(\"Reshaping observations array.\")\n",
    "        y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "\n",
    "    # first, compute means\n",
    "    y_bar = np.mean(y_train)\n",
    "    x_bar = np.mean(x_train)\n",
    "\n",
    "    # build the two terms\n",
    "    numerator = np.sum( (x_train - x_bar)*(y_train - y_bar) )\n",
    "    denominator = np.sum((x_train - x_bar)**2)\n",
    "    \n",
    "    #slope beta1\n",
    "    beta_1 = numerator/denominator\n",
    "\n",
    "    #intercept beta0\n",
    "    beta_0 = y_bar - beta_1*x_bar\n",
    "\n",
    "    return np.array([beta_0,beta_1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's run this function and see the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1 ,2, 3])\n",
    "y_train = np.array([2, 2, 4])\n",
    "\n",
    "betas = simple_linear_regression_fit(x_train, y_train)\n",
    "\n",
    "beta_0 = betas[0]\n",
    "beta_1 = betas[1]\n",
    "\n",
    "print(\"The best-fit line is {0:8.6f} + {1:8.6f} * x\".format(beta_0, beta_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "* Do the values of `beta0` and `beta1` seem reasonable?\n",
    "* Plot the training data using a scatter plot.\n",
    "* Plot the best fit line with `beta0` and `beta1` together with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/best_fit_scatterplot.py\n",
    "fig_scat, ax_scat = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "# Plot best-fit line\n",
    "x_train = np.array([[1, 2, 3]]).T\n",
    "\n",
    "best_fit = beta_0 + beta_1 * x_train\n",
    "\n",
    "ax_scat.scatter(x_train, y_train, s=300, label='Training Data')\n",
    "ax_scat.plot(x_train, best_fit, ls='--', label='Best Fit Line')\n",
    "\n",
    "ax_scat.set_xlabel(r'$x_{train}$')\n",
    "ax_scat.set_ylabel(r'$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of `beta0` and `beta1` seem roughly reasonable.  They capture the positive correlation.  The line does appear to be trying to get as close as possible to all the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "## 3 - Building a model with `statsmodels` and `sklearn`\n",
    "\n",
    "Now that we can concretely fit the training data from scratch, let's learn two `python` packages to do it all for us:\n",
    "* [statsmodels](http://www.statsmodels.org/stable/regression.html) and \n",
    "* [scikit-learn (sklearn)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "Our goal  is to show how to implement simple linear regression with these packages.  For an important sanity check, we compare the $\\beta$ values from `statsmodels` and `sklearn` to the $\\beta$ values that we found from above with our own implementation.\n",
    "\n",
    "For the purposes of this lab, `statsmodels` and `sklearn` do the same thing.  More generally though, `statsmodels` tends to be easier for inference \\[finding the values of the slope and intercept and dicussing uncertainty in those values\\], whereas `sklearn` has machine-learning algorithms and is better for prediction \\[guessing y values for a given x value\\]. (Note that both packages make the same guesses, it's just a question of which activity they provide more support for.\n",
    "\n",
    "**Note:** `statsmodels` and `sklearn` are different packages!  Unless we specify otherwise, you can use either one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for `statsmodels`.  `Statsmodels` does not by default include the column of ones in the $X$ matrix, so we include it manually with `sm.add_constant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the X matrix by appending a column of ones to x_train\n",
    "X = sm.add_constant(x_train)\n",
    "\n",
    "# this is the same matrix as in our scratch problem!\n",
    "print(X)\n",
    "\n",
    "# build the OLS model (ordinary least squares) from the training data\n",
    "toyregr_sm = sm.OLS(y_train, X)\n",
    "\n",
    "# do the fit and save regression info (parameters, etc) in results_sm\n",
    "results_sm = toyregr_sm.fit()\n",
    "\n",
    "# pull the beta parameters out from results_sm\n",
    "beta0_sm = results_sm.params[0]\n",
    "beta1_sm = results_sm.params[1]\n",
    "\n",
    "print(f'The regression coef from statsmodels are: beta_0 = {beta0_sm:8.6f} and beta_1 = {beta1_sm:8.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the beta parameters, `results_sm` contains a ton of other potentially useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "print(results_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn our attention to the `sklearn` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the least squares model\n",
    "toyregr = linear_model.LinearRegression()\n",
    "\n",
    "# save regression info (parameters, etc) in results_skl\n",
    "results = toyregr.fit(x_train, y_train)\n",
    "\n",
    "# pull the beta parameters out from results_skl\n",
    "beta0_skl = toyregr.intercept_\n",
    "beta1_skl = toyregr.coef_[0]\n",
    "\n",
    "print(\"The regression coefficients from the sklearn package are: beta_0 = {0:8.6f} and beta_1 = {1:8.6f}\".format(beta0_skl, beta1_skl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `scikit-learn` library and the shape of things\n",
    "\n",
    "Before diving into a \"real\" problem, let's discuss more of the details of `sklearn`.\n",
    "\n",
    "`Scikit-learn` is the main `Python` machine learning library. It consists of many learners which can learn models from data, as well as a lot of utility functions such as `train_test_split()`. \n",
    "\n",
    "Use the following to add the library into your code:\n",
    "\n",
    "```python\n",
    "import sklearn \n",
    "```\n",
    "\n",
    "In `scikit-learn`, an **estimator** is a Python object that implements the methods `fit(X, y)` and `predict(T)`\n",
    "\n",
    "Let's see the structure of `scikit-learn` needed to make these fits. `fit()` always takes two arguments:\n",
    "```python\n",
    "estimator.fit(Xtrain, ytrain)\n",
    "```\n",
    "We will consider two estimators in this lab: `LinearRegression` and `KNeighborsRegressor`.\n",
    "\n",
    "It is very important to understand that `Xtrain` must be in the form of a **2x2 array** with each row corresponding to one sample, and each column corresponding to the feature values for that sample.\n",
    "\n",
    "`ytrain` on the other hand is a simple array of responses.  These are continuous for regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice with `sklearn` and a real dataset\n",
    "We begin by loading up the `mtcars` dataset. This data was extracted from the 1974 Motor Trend US magazine, and comprises of fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models). We will load this data to a dataframe with 32 observations on 11 (numeric) variables. Here is an explanation of the features:\n",
    "\n",
    "- `mpg` is Miles/(US) gallon \n",
    "- `cyl` is Number of cylinders, \n",
    "- `disp` is\tDisplacement (cu.in.), \n",
    "- `hp` is\tGross horsepower, \n",
    "- `drat` is\tRear axle ratio, \n",
    "- `wt` is the Weight (1000 lbs), \n",
    "- `qsec` is 1/4 mile time,\n",
    "- `vs` is Engine (0 = V-shaped, 1 = straight), \n",
    "- `am` is Transmission (0 = automatic, 1 = manual), \n",
    "- `gear` is the Number of forward gears, \n",
    "- `carb` is\tNumber of carburetors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mtcars\n",
    "dfcars = pd.read_csv(\"data/mtcars.csv\")\n",
    "dfcars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the column title \n",
    "dfcars = dfcars.rename(columns={\"Unnamed: 0\":\"car name\"})\n",
    "dfcars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching for values: how many cars have 4 gears?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(dfcars[dfcars.gear == 4].drop_duplicates(subset='car name', keep='first'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's split the dataset into a training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random_state to get the same split every time\n",
    "traindf, testdf = train_test_split(dfcars, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing set is around 20% of the total data; training set is around 80%\n",
    "print(\"Shape of full dataset is: {0}\".format(dfcars.shape))\n",
    "print(\"Shape of training dataset is: {0}\".format(traindf.shape))\n",
    "print(\"Shape of test dataset is: {0}\".format(testdf.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have training and test data.  We still need to select a predictor and a response from this dataset.  Keep in mind that we need to choose the predictor and response from both the training and test set.  You will do this in the exercises below.  However, we provide some starter code for you to get things going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the response variable that we're interested in\n",
    "y_train = traindf.mpg\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice the shape of `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, type(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array reshape\n",
    "This is a 1D array as should be the case with the **Y** array.  Remember, `sklearn` requires a 2D array only for the predictor array.  You will have to pay close attention to this in the exercises later. `Sklearn` doesn't care too much about the shape of `y_train`.\n",
    "\n",
    "The whole reason we went through that whole process was to show you how to reshape your data into the correct format.\n",
    "\n",
    "**IMPORTANT:** Remember that your response variable `ytrain` can be a vector but your predictor variable `xtrain` ***must*** be an array!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "## 3 - Example: Simple linear regression with automobile data\n",
    "We will now use `sklearn` to predict automobile mileage per gallon (mpg) and evaluate these predictions. We already loaded the data and split them into a training set and a test set.\n",
    "\n",
    "We need to choose the variables that we think will be good predictors for the dependent variable `mpg`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "* Pick one variable to use as a predictor for simple linear regression.  Discuss your reasons with the person next to you.  \n",
    "* Justify your choice with some visualizations.  \n",
    "* Is there a second variable you'd like to use? For example, we're not doing multiple linear regression here, but if we were, is there another variable you'd like to include if we were using two predictors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cars_simple_EDA.py\n",
    "y_mpg = dfcars.mpg\n",
    "x_wt = dfcars.wt\n",
    "x_hp = dfcars.hp\n",
    "\n",
    "fig_wt, ax_wt = plt.subplots(1,1, figsize=(10,6))\n",
    "ax_wt.scatter(x_wt, y_mpg)\n",
    "ax_wt.set_xlabel(r'Car Weight')\n",
    "ax_wt.set_ylabel(r'Car MPG')\n",
    "\n",
    "fig_hp, ax_hp = plt.subplots(1,1, figsize=(10,6))\n",
    "ax_hp.scatter(x_hp, y_mpg)\n",
    "ax_hp.set_xlabel(r'Car HP')\n",
    "ax_hp.set_ylabel(r'Car MPG')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "* Use `sklearn` to fit the training data using simple linear regression.\n",
    "* Use the model to make mpg predictions on the test set.  \n",
    "* Plot the data and the prediction.  \n",
    "* Print out the mean squared error for the training set and the test set and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcars = pd.read_csv('data/mtcars.csv')\n",
    "dfcars = dfcars.rename(columns={\"Unnamed: 0\":\"name\"})\n",
    "\n",
    "dfcars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf, testdf = train_test_split(dfcars, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = np.array(traindf.mpg)\n",
    "X_train = np.array(traindf.wt)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(testdf.mpg)\n",
    "X_test = np.array(testdf.wt)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take another look at our data\n",
    "dfcars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And out train and test sets \n",
    "y_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create linear model\n",
    "regression = LinearRegression()\n",
    "\n",
    "#fit linear model\n",
    "regression.fit(X_train, y_train)\n",
    "\n",
    "predicted_y = regression.predict(X_test)\n",
    "\n",
    "r2 = regression.score(X_test, y_test)\n",
    "print(f'R^2 = {r2:.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regression.score(X_train, y_train))\n",
    "\n",
    "print(mean_squared_error(predicted_y, y_test))\n",
    "print(mean_squared_error(y_train, regression.predict(X_train)))\n",
    "\n",
    "print('Coefficients: \\n', regression.coef_[0], regression.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.plot(y_test, predicted_y, 'o')\n",
    "grid = np.linspace(np.min(dfcars.mpg), np.max(dfcars.mpg), 100)\n",
    "ax.plot(grid, grid, color=\"black\") # 45 degree line\n",
    "ax.set_xlabel(\"actual y\")\n",
    "ax.set_ylabel(\"predicted y\")\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1, figsize=(10,6))\n",
    "ax1.plot(dfcars.wt, dfcars.mpg, 'o')\n",
    "xgrid = np.linspace(np.min(dfcars.wt), np.max(dfcars.wt), 100)\n",
    "ax1.plot(xgrid, regression.predict(xgrid.reshape(100, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression, and Cab Data\n",
    "\n",
    "Polynomial regression uses a **linear model** to estimate a **non-linear function** (i.e., a function with polynomial terms). For example:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_i + \\beta_1x_i^{2}$\n",
    "\n",
    "It is a linear model because we are still solving a linear equation (the _linear_ aspect refers to the beta coefficients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data, break into train and test\n",
    "cab_df = pd.read_csv(\"data/cabs.txt\")\n",
    "train_data, test_data = train_test_split(cab_df, test_size=.2, random_state=42)\n",
    "cab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some data cleaning\n",
    "X_train = train_data['TimeMin'].values.reshape(-1,1)/60 # transforms it to being hour-based\n",
    "y_train = train_data['PickupCount'].values\n",
    "\n",
    "X_test = test_data['TimeMin'].values.reshape(-1,1)/60 # hour-based\n",
    "y_test = test_data['PickupCount'].values\n",
    "\n",
    "def plot_cabs(cur_model, poly_transformer=None):\n",
    "    \n",
    "    # build the x values for the prediction line\n",
    "    x_vals = np.arange(0,24,.1).reshape(-1,1)\n",
    "    \n",
    "    # optionally use the passed-in transformer\n",
    "    if poly_transformer != None:\n",
    "        dm = poly_transformer.fit_transform(x_vals)\n",
    "    else:\n",
    "        dm = x_vals\n",
    "        \n",
    "    # make the prediction at each x value\n",
    "    prediction = cur_model.predict(dm)\n",
    "    \n",
    "    # plot the prediction line, and the test data\n",
    "    plt.plot(x_vals,prediction, color='k', label=\"Prediction\")\n",
    "    plt.scatter(X_test, y_test, label=\"Test Data\")\n",
    "\n",
    "    # label your plots\n",
    "    plt.ylabel(\"Number of Taxi Pickups\")\n",
    "    plt.xlabel(\"Time of Day (Hours Past Midnight)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_cab_model0 = LinearRegression().fit(X_train, y_train)\n",
    "plot_cabs(fitted_cab_model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_cab_model0.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "**Questions**:\n",
    "1. The above code uses `sklearn`. As more practice, and to help you stay versed in both libraries, perform the same task (fit a linear regression line) using `statsmodels` and report the $r^2$ score. Is it the same value as what sklearn reports, and is this the expected behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION:\n",
    "\n",
    "# augment the data with a column vector of 1's\n",
    "train_data_augmented = sm.add_constant(X_train)\n",
    "test_data_augmented = sm.add_constant(X_test)\n",
    "\n",
    "# fit the model on the training data\n",
    "OLSModel = OLS(train_data['PickupCount'].values, train_data_augmented).fit()\n",
    "\n",
    "# get the prediction results\n",
    "ols_predicted_pickups_test = OLSModel.predict(test_data_augmented)\n",
    "r2_score_test = r2_score(test_data[['PickupCount']].values, ols_predicted_pickups_test)\n",
    "print(r2_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there's still a lot of variation in cab pickups that's not being captured by a linear fit. Further, the linear fit is predicting massively more pickups at 11:59pm than at 12:00am. This is a bad property, and it's the conseqeuence of having a straight line with a non-zero slope. However, we can add columns to our data for $TimeMin^2$ and $TimeMin^3$ and so on, allowing a curvy polynomial line to hopefully fit the data better.\n",
    "\n",
    "We'll be using ``sklearn``'s `PolynomialFeatures()` function to take some of the tedium out of building the expanded input data. In fact, if all we want is a formula like $y \\approx \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ...$, it will directly return a new copy of the data in this format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_3 = PolynomialFeatures(3, include_bias=False)\n",
    "expanded_train = transformer_3.fit_transform(X_train) # TRANSFORMS it to polynomial features\n",
    "pd.DataFrame(expanded_train).describe() # notice that the columns now contain x, x^2, x^3 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on `PolynomialFeatures`:\n",
    "\n",
    "- The interface is a bit strange. `PolynomialFeatures` is a _'transformer'_ in sklearn. We'll be using several transformers that learn a transformation on the training data, and then we will apply those transformations on future data. With PolynomialFeatures, the `.fit()` is pretty trivial, and we often fit and transform in one command, as seen above with ``.fit_transform()`.\n",
    "- You rarely want to `include_bias` (a column of all 1's), since _**sklearn**_ will add it automatically. Remember, when using _**statsmodels,**_ you can just `.add_constant()` right before you fit the data.\n",
    "- If you want polynomial features for a several different variables (i.e., multinomial regression), you should call `.fit_transform()` separately on each column and append all the results to a copy of the data (unless you also want interaction terms between the newly-created features). See `np.concatenate()` for joining arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_cab_model3 = LinearRegression().fit(expanded_train, y_train)\n",
    "print(\"fitting expanded_train:\", expanded_train)\n",
    "plot_cabs(fitted_cab_model3, transformer_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "**Questions**:\n",
    "1. Calculate the polynomial model's $R^2$ performance on the test set. \n",
    "2. Does the polynomial model improve on the purely linear model?\n",
    "3. Make a residual plot for the polynomial model. What does this plot tell us about the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 1\n",
    "expanded_test = transformer_3.fit_transform(X_test)\n",
    "print(\"Test R-squared:\", fitted_cab_model3.score(expanded_test, y_test))\n",
    "# NOTE 1: unlike statsmodels' r2_score() function, sklearn has a .score() function\n",
    "# NOTE 2: fit_transform() is a nifty function that transforms the data, then fits it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 2: yes it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER 3 (class discussion about the residuals)\n",
    "x_matrix = transformer_3.fit_transform(X_train)\n",
    "\n",
    "prediction = fitted_cab_model3.predict(x_matrix)\n",
    "residual = y_train - prediction\n",
    "plt.scatter(X_train, residual, label=\"Residual\")\n",
    "plt.axhline(0, color='k')\n",
    "\n",
    "plt.title(\"Residuals for the Cubic Model\")\n",
    "plt.ylabel(\"Residual Number of Taxi Pickups\")\n",
    "plt.xlabel(\"Time of Day (Hours Past Midnight)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other features\n",
    "Polynomial features are not the only constucted features that help fit the data. Because these data have a 24 hour cycle, we may want to build features that follow such a cycle. For example, $sin(24\\frac{x}{2\\pi})$, $sin(12\\frac{x}{2\\pi})$, $sin(8\\frac{x}{2\\pi})$. Other feature transformations are appropriate to other types of data. For instance certain feature transformations have been developed for geographical data.\n",
    "\n",
    "### Scaling Features\n",
    "When using polynomials, we are explicitly trying to use the higher-order values for a given feature. However, sometimes these polynomial features can take on values that are drastically large, making it difficult for the system to learn an appropriate bias weight due to its large values and potentially large variance. To counter this, sometimes one may be interested in scaling the values for a given feature.\n",
    "\n",
    "For our ongoing taxi-pickup example, using polynomial features improved our model. If we wished to scale the features, we could use `sklearn`'s StandardScaler() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALES THE EXPANDED/POLY TRANSFORMED DATA\n",
    "# we don't need to convert to a pandas dataframe, but it can be useful for scaling select columns\n",
    "train_copy = pd.DataFrame(expanded_train.copy())\n",
    "test_copy = pd.DataFrame(expanded_test.copy())\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler = StandardScaler().fit(train_copy)\n",
    "\n",
    "# Scale both the test and training data. \n",
    "train_scaled = scaler.transform(expanded_train)\n",
    "test_scaled = scaler.transform(expanded_test)\n",
    "\n",
    "# we could optionally run a new regression model on this scaled data\n",
    "fitted_scaled_cab = LinearRegression().fit(train_scaled, y_train)\n",
    "fitted_scaled_cab.score(test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px\">\n",
    "\n",
    "## Multiple regression and exploring the Football (aka soccer) data\n",
    "Let's move on to a different dataset! The data imported below were scraped by [Shubham Maurya](https://www.kaggle.com/mauryashubham/linear-regression-to-predict-market-value/data) and record various facts about players in the English Premier League. Our goal will be to fit models that predict the players' market value (what the player could earn when hired by a new team), as estimated by https://www.transfermarkt.us.\n",
    "\n",
    "`name`: Name of the player  \n",
    "`club`: Club of the player  \n",
    "`age` : Age of the player  \n",
    "`position` : The usual position on the pitch  \n",
    "`position_cat` :  1 for attackers, 2 for midfielders, 3 for defenders, 4 for goalkeepers  \n",
    "`market_value` : As on www.transfermarkt.us.on July 20th, 2017  \n",
    "`page_views` : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017  \n",
    "`fpl_value` : Value in Fantasy Premier League as on July 20th, 2017  \n",
    "`fpl_sel` : % of FPL players who have selected that player in their team  \n",
    "`fpl_points` : FPL points accumulated over the previous season  \n",
    "`region`: 1 for England, 2 for EU, 3 for Americas, 4 for Rest of World  \n",
    "`nationality`: Player's nationality  \n",
    "`new_foreign`: Whether a new signing from a different league, for 2017/18 (till 20th July)  \n",
    "`age_cat`: a categorical version of the Age feature  \n",
    "`club_id`: a numerical version of the Club feature  \n",
    "`big_club`: Whether one of the Top 6 clubs  \n",
    "`new_signing`: Whether a new signing for 2017/18 (till 20th July)  \n",
    "\n",
    "As always, we first import, verify, split, and explore the data.\n",
    "\n",
    "## Part 1: Import and verification and grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_df = pd.read_csv(\"data/league_data.txt\")\n",
    "print(league_df.dtypes)\n",
    "\n",
    "# QUESTION: what would you guess is the mean age? mean salary?\n",
    "league_df.head() # turns out, it's a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "league_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Stratified) train/test split\n",
    "We want to make sure that the training and test data have appropriate representation of each region; it would be bad for the training data to entirely miss a region. This is especially important because some regions are rather rare.\n",
    "\n",
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "**Questions**:\n",
    "1. Use the `train_test_split()` function, while (a) ensuring the test size is 20% of the data, and; (2) using 'stratify' argument to split the data (look up documentation online), keeping equal representation of each region. This doesn't work by default, correct? What is the issue?\n",
    "2. Deal with the issue you encountered above. Hint: you may find numpy's `.isnan()` and panda's `.dropna()` functions useful!\n",
    "3. How did you deal with the error generated by `train_test_split`? How did you justify your action? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION:\n",
    "try:\n",
    "    \n",
    "    # Doesn't work: a value is missing\n",
    "    train_data, test_data = train_test_split(league_df, test_size = 0.2, stratify=league_df['region'])\n",
    "except:\n",
    "    \n",
    "    # Count the missing lines and drop them\n",
    "    missing_rows = np.isnan(league_df['region'])\n",
    "    print(\"Uh oh, {} lines missing data! Dropping them\".format(np.sum(missing_rows)))\n",
    "    league_df = league_df.dropna(subset=['region'])\n",
    "    train_data, test_data = train_test_split(league_df, test_size = 0.2, stratify=league_df['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we won't be peeking at the test set, let's explore and look for patterns! We'll introduce a number of useful pandas and numpy functions along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "Pandas' `.groupby()` function is a wonderful tool for data analysis. It allows us to analyze each of several subgroups.\n",
    "\n",
    "Many times, `.groupby()` is combined with `.agg()` to get a summary statistic for each subgroup. For instance: What is the average market value, median page views, and maximum fpl for each player position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby('position').agg({\n",
    "    'market_value': np.mean,\n",
    "    'page_views': np.median,\n",
    "    'fpl_points': np.max\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.position.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(['big_club', 'position']).agg({\n",
    "    'market_value': np.mean,\n",
    "    'page_views': np.mean,\n",
    "    'fpl_points': np.mean\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "**Question**:\n",
    "1. Notice that the `.groupby()` function above takes a list of two column names. Does the order matter? What happens if we switch the two so that 'position' is listed before 'big_club'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION:\n",
    "train_data.groupby(['position', 'big_club']).agg({\n",
    "    'market_value': np.mean,\n",
    "    'page_views': np.mean,\n",
    "    'fpl_points': np.mean\n",
    "})\n",
    "\n",
    "# in this case, our values are the same, as we are not aggregating anything differently;\n",
    "# however, our view / grouping is merely different. visually, it often makes most sense to\n",
    "# group such that the left-most (earlier) groupings have fewer distinct options than\n",
    "# the ones to the right of it, but it all depends on what you're trying to discern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px\">\n",
    "\n",
    "## Part 2: Linear regression on the football data\n",
    "This section of the lab focuses on fitting a model to the football (soccer) data and interpreting the model results. The model we'll use is\n",
    "\n",
    "$$\\text{market_value} \\approx \\beta_0 + \\beta_1\\text{fpl_points} + \\beta_2\\text{age} + \\beta_3\\text{age}^2 + \\beta_4log_2\\left(\\text{page_views}\\right) + \\beta_5\\text{new_signing} +\\beta_6\\text{big_club} + \\beta_7\\text{position_cat}$$\n",
    "\n",
    "We're including a 2nd degree polynomial in age because we expect pay to increase as a player gains experience, but then decrease as they continue aging. We're taking the log of page views because they have such a large, skewed range and the transformed variable will have fewer outliers that could bias the line. We choose the base of the log to be 2 just to make interpretation cleaner.\n",
    "\n",
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "**Questions**:\n",
    "1. Build the data and fit this model to it. How good is the overall model?\n",
    "2. Interpret the regression model. What is the meaning of the coefficient for:\n",
    "    - age and age$^2$\n",
    "    - $log_2($page_views$)$\n",
    "    - big_club\n",
    "3. What should a player do in order to improve their market value? How many page views should a player go get to increase their market value by 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: we'll do most of it for you ...\n",
    "y_train = train_data['market_value']\n",
    "y_test = test_data['market_value']\n",
    "def build_football_data(df):\n",
    "    x_matrix = df[['fpl_points','age','new_signing','big_club','position_cat']].copy()\n",
    "    x_matrix['log_views'] = np.log2(df['page_views'])\n",
    "    \n",
    "    # CREATES THE AGE SQUARED COLUMN\n",
    "    x_matrix['age_squared'] = df['age']**2\n",
    "    \n",
    "    # OPTIONALLY WRITE CODE to adjust the ordering of the columns, just so that it corresponds with the equation above\n",
    "    x_matrix = x_matrix[['fpl_points','age','age_squared','log_views','new_signing','big_club','position_cat']]\n",
    "    \n",
    "    # add a constant\n",
    "    x_matrix = sm.add_constant(x_matrix)\n",
    "    \n",
    "    return x_matrix\n",
    "\n",
    "# use build_football_data() to transform both the train_data and test_data\n",
    "train_transformed = build_football_data(train_data)\n",
    "test_transformed = build_football_data(test_data)\n",
    "\n",
    "fitted_model_1 = OLS(endog= y_train, exog=train_transformed, hasconst=True).fit()\n",
    "fitted_model_1.summary()\n",
    "\n",
    "# WRITE CODE TO RUN r2_score(), then answer the above question about the overall goodness of the model\n",
    "r2_score(y_test, fitted_model_1.predict(test_transformed))\n",
    "\n",
    "# The model is reasonably good. We're capturing about 64%-69% of the variation in market values,\n",
    "# and the test set confirms that we're not overfitting too badly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: let's use the age coefficients to show the effect of age has on one's market value;\n",
    "# we can get the age and age^2 coefficients via:\n",
    "agecoef = fitted_model_1.params.age\n",
    "age2coef = fitted_model_1.params.age_squared\n",
    "\n",
    "# let's set our x-axis (corresponding to age) to be a wide range from -100 to 100, \n",
    "# just to see a grand picture of the function\n",
    "x_vals = np.linspace(-100,100,1000)\n",
    "y_vals = agecoef*x_vals +age2coef*x_vals**2\n",
    "\n",
    "# WRITE CODE TO PLOT x_vals vs y_vals\n",
    "plt.plot(x_vals, y_vals)\n",
    "plt.title(\"Effect of Age\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Contribution to Predicted Market Value\")\n",
    "plt.show()\n",
    "\n",
    "# Q2A: WHAT HAPPENS IF WE USED ONLY AGE (not AGE^2) in our model (what's the r2?); make the same plot of age vs market value\n",
    "# Q2B: WHAT HAPPENS IF WE USED ONLY AGE^2 (not age) in our model (what's the r2?); make the same plot of age^2 vs market value\n",
    "# Q2C: PLOT page views vs market value\n",
    "\n",
    "# SOLUTION\n",
    "page_view_coef = fitted_model_1.params.log_views\n",
    "x_vals = np.linspace(0,15)\n",
    "y_vals = page_view_coef*x_vals\n",
    "plt.plot(x_vals, y_vals)\n",
    "plt.title(\"Effect of Page Views\")\n",
    "plt.xlabel(\"Page Views\")\n",
    "plt.ylabel(\"Contribution to Predicted Market Value\")\n",
    "plt.show()\n",
    "\n",
    "# 3- Linear regression on non-experimental data can't determine causation, so we can't prove that\n",
    "# a given relationship runs in the direction we might think. For instance, doing whatever it\n",
    "# takes to get more page views probably doesn't meaningfully increase market value; it's likely\n",
    "# the causation runs in the other direction and great players get more views. Even so, we can use\n",
    "# page views to help us tell who is a great player and thus likely to be paid well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3px'>\n",
    "\n",
    "### Part 3: Turning Categorical Variables into multiple binary variables\n",
    "Of course, we have an error in how we've included player position. Even though the variable is numeric (1,2,3,4) and the model runs without issue, the value we're getting back is garbage. The interpretation, such as it is, is that there is an equal effect of moving from position category 1 to 2, from 2 to 3, and from 3 to 4, and that this effect is probably between -0.5 to -1 (depending on your run).\n",
    "\n",
    "In reality, we don't expect moving from one position category to another to be equivalent, nor for a move from category 1 to category 3 to be twice as important as a move from category 1 to category 2. We need to introduce better features to model this variable.\n",
    "\n",
    "We'll use `pd.get_dummies` to do the work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_design_recoded = pd.get_dummies(train_transformed, columns=['position_cat'], drop_first=True)\n",
    "test_design_recoded = pd.get_dummies(test_transformed, columns=['position_cat'], drop_first=True)\n",
    "\n",
    "train_design_recoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've removed the original `position_cat` column and created three new ones.\n",
    "\n",
    "#### Why only three new columns?\n",
    "Why does pandas give us the option to drop the first category? \n",
    "\n",
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "**Questions**:\n",
    "1. If we're fitting a model without a constant, should we have three dummy columns or four dummy columns?\n",
    "2. Fit a model on the new, recoded data, then interpret the coefficient of `position_cat_2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION:\n",
    "resu = OLS(y_train, train_design_recoded).fit()\n",
    "resu.summary()\n",
    "print(\"r2:\", r2_score(y_test, resu.predict(test_design_recoded)))\n",
    "print(\"position_cat_2 coef:\", resu.params.position_cat_2)\n",
    "train_design_recoded.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:**\n",
    "1. If our model does not have a constant, we must include all four dummy variable columns. If we drop one, we're not modeling any effect of being in that category, and effectively assuming the dropped category's effect is 0.\n",
    "2. Being in position 2 (instead of position 1) has an impact between -1.54 and +2.38 on a player's market value. Since we're using an intercept, the dropped category becomes the baseline and the effect of any dummy variable is the effect of being in that category instead of the baseline category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A nice trick for forward-backwards\n",
    "\n",
    "XOR (operator ^) is a logical operation that only returns true when input differ. We can use it to implement forward-or-backwards selection when we want to keep track of what predictors are \"left\" from a given list of predictors.\n",
    "\n",
    "The set analog is \"symmetric difference\". From the python docs:\n",
    "\n",
    "`s.symmetric_difference(t)\ts ^ t\tnew set with elements in either s or t but not both`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set() ^ set([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([1]) ^ set([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([1, 2]) ^ set([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "Outline a step-forwards algorithm which uses this idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:**\n",
    "\n",
    "Start with no predictors in a set, `selected_predictors`. Then the \"xor\" will give the set of all predictors. Go through them 1-by -1, seeing which has the highest score/ OR lowestaic/bic. Add this predictor to the `selected_predictors`.\n",
    "\n",
    "Now repeat. The xor will eliminate this predictor from the remaining predictors. In the next iteration we will pick the next predictor which when combined with the first one gibes the lowest aic/bic of all 2-predictor models.\n",
    "\n",
    "We repeat. We finally chose the best bic model from the 1 -predictor models, 2-predictor models, 3-predictor models and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS EXERCISE:\n",
    "We have provided a spreadsheet of Boston housing prices (data/boston_housing.csv). The 14 columns are as follows:\n",
    "1. CRIM: per capita crime rate by town\n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS: proportion of non-retail business acres per town\n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. NOX: nitric oxides concentration (parts per 10 million)\n",
    "6. RM: average number of rooms per dwelling\n",
    "7. AGE: proportion of owner-occupied units built prior to 1940\n",
    "8. DIS: weighted distances to ﬁve Boston employment centers\n",
    "9. RAD: index of accessibility to radial highways\n",
    "10. TAX: full-value property-tax rate per \\$10,000\n",
    "11. PTRATIO: pupil-teacher ratio by town\n",
    "12. B: 1000(Bk−0.63)2 where Bk is the proportion of blacks by town\n",
    "13. LSTAT: % lower status of the population\n",
    "14. MEDV: Median value of owner-occupied homes in $1000s We can see that the input attributes have a mixture of units\n",
    "\n",
    "There are 450 observations.\n",
    "<div class=\"exercise\"><b>Exercise</b></div>\n",
    "\n",
    "Using the above file, try your best to predict **housing prices. (the 14th column)** We have provided a test set `data/boston_housing_test.csv` but refrain from looking at the file or evaluating on it until you have finalized and trained a model.\n",
    "1. Load in the data. It is tab-delimited. Quickly look at a summary of the data to familiarize yourself with it and ensure nothing is too egregious.\n",
    "2. Use a previously-discussed function to automatically partition the data into a training and validation (aka development) set. It is up to you to choose how large these two portions should be.\n",
    "3. Train a basic model on just a subset of the features. What is the performance on the validation set?\n",
    "4. Train a basic model on all of the features. What is the performance on the validation set?\n",
    "5. Toy with the model until you feel your results are reasonably good.\n",
    "6. Perform cross-validation with said model, and measure the average performance. Are the results what you expected? Were the average results better or worse than that from your original 1 validation set?\n",
    "7. Experiment with other models, and for each, perform 10-fold cross-validation. Which model yields the best average performance? Select this as your final model.\n",
    "8. Use this model to evaulate your performance on the testing set. What is your performance (MSE)? Is this what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57990f1ad2ea89c67ddae7f31d40c478205c5912da0fccfb7c5cfbb2b8bf17ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
