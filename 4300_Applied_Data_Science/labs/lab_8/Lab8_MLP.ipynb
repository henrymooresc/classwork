{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='heading'>\n",
    "    <div style='float:left;'><h1>CPSC 4300/6300: Applied Data Science</h1></div>\n",
    "    <img style=\"float: right; padding-right: 10px; width: 65px\" src=\"https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/clemson_paw.png\"> </div>\n",
    "\n",
    "\n",
    "## Week 10: Introduction to Neural Networks\n",
    "\n",
    "**Clemson University**<br>\n",
    "**Fall 2021**<br>\n",
    "**Instructor(s):** Nina Hubig <br>\n",
    "**Author(s):** Brandon Walker\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RUN THIS CELL TO GET THE RIGHT FORMATTING \"\"\"\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "css_file = 'https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/css/cpsc6300.css'\n",
    "styles = requests.get(css_file).text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "import urllib\n",
    "import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "\n",
    "print(tf.__version__)  # You should see a 2.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions for running `tf.keras` with Tensorflow 2.4:  \n",
    "\n",
    "1. Create a `conda` virtual environment by cloning an existing one that you know works\n",
    "```\n",
    "conda create --name myclone --clone myenv\n",
    "```\n",
    "\n",
    "2. Go to [https://www.tensorflow.org/install](https://www.tensorflow.org/install) and follow instructions for your machine.\n",
    "\n",
    "All references to Keras should be written as `tf.keras`.  For example: \n",
    "\n",
    "```\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "tf.keras.models.Sequential\n",
    "tf.keras.layers.Dense, tf.keras.layers.Activation, \n",
    "tf.keras.layers.Dropout, tf.keras.layers.Flatten, tf.keras.layers.Reshape\n",
    "tf.keras.optimizers.SGD\n",
    "tf.keras.preprocessing.image.ImageDataGenerator\n",
    "tf.keras.regularizers\n",
    "tf.keras.datasets.mnist   \n",
    "```\n",
    "\n",
    "You could avoid the long names by using\n",
    "```\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "```\n",
    "These imports do not work on some systems, however, because they pick up previous versions of `keras` and `tensorflow`. That is why I avoid them in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "In this lab we will understand the basics of neural networks and how to start using a deep learning library called `keras`. By the end of this lab, you should:\n",
    "\n",
    "- Understand how a simple neural network works and code some of its functionality from scratch.\n",
    "- Be able to think and do calculations in matrix notation. Also think of vectors and arrays as tensors.\n",
    "- Know how to install and run `tf.keras`.\n",
    "- Implement a simple real world example using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Neural Networks 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have an input vector $X=${$x_1, x_2, ... x_L$} to a $k$-layered network. <BR><BR>\n",
    "Each layer has its own number of nodes. For the first layer in our drawing that number is $J$. We can store the weights for each node in a vector $\\mathbf{W} \\in \\mathbb{R}^{JxL+1}$ (accounting for bias).  Similarly, we can store the biases from each node in a vector $\\mathbf{b} \\in \\mathbb{R}^{I}$.  The affine transformation is then written as $$\\mathbf{a} = \\mathbf{W^T}X + \\mathbf{b}$$ <BR>  What we then do is \"absorb\" $\\mathbf{b}$ into $X$ by adding a column of ones to $X$. Our $X$ matrix than becomes $\\mathbf{X} \\in \\mathbb{R}^{JxL+1}$ and our equation: <BR><BR>$$\\mathbf{a} = \\mathbf{W^T}_{plusones}X$$ <br>We have that $\\mathbf{a} \\in \\mathbb{R}^{J}$ as well.  Next we evaluate the output from each node.  We write $$\\mathbf{u} = \\sigma\\left(\\mathbf{a}\\right)$$ where $\\mathbf{u}\\in\\mathbb{R}^{J}$.  We can think of $\\sigma$ operating on each individual element of $\\mathbf{a}$ separately or in matrix notation. If we denote each component of $\\mathbf{a}$ by $a_{j}$ then we can write $$u_{j} = \\sigma\\left(a_{j}\\right), \\quad j = 1, ... J.$$<br> In our code we will implement all these equations in matrix notation.\n",
    "`tf.keras` (Tensorflow) and `numpy` perform the calculations in matrix format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width: 1024px\" src=\"https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/neuralnet.png\">\n",
    "<br>\n",
    "*Image source: \"Modern Mathematical Methods for Computational Science and Engineering\" Efthimios Kaxiras and Athanassios Fokas.*\n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we have 3 input points (L = 3), two hidden layers ($k=2$), and 2 nodes in each layer ($J=2$)<br>\n",
    "\n",
    "### Input Layer\n",
    "\n",
    "$ùëã$={$ùë•_1,ùë•_2,x_3$}\n",
    "\n",
    "### First Hidden Layer\n",
    "\n",
    "\\begin{equation}\n",
    " \\begin{aligned}\n",
    "a^{(1)}_1 = w^{(1)}_{10} + w^{(1)}_{11}x_1 + w^{(1)}_{12}x_2 + w^{(1)}_{13}x_3 \\\\\n",
    "a^{(1)}_2 = w^{(1)}_{20} + w^{(1)}_{21}x_1 + w^{(1)}_{22}x_2 + w^{(1)}_{23}x_3 \\\\ \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "<br> All this in matrix notation: $$\\mathbf{a} = \\mathbf{W^T}X$$\n",
    "<br> NOTE: in $X$ we have added a column of ones to account for the bias<BR><BR>\n",
    "**Then the sigmoid is applied**:\n",
    "\\begin{equation}\n",
    " \\begin{aligned}\n",
    "u^{(1)}_1 = \\sigma(a^{(1)}_1) \\\\\n",
    "u^{(1)}_2 = \\sigma(a^{(1)}_2) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "    \n",
    "or in matrix notation: $$\\mathbf{u} = \\sigma\\left(\\mathbf{a}\\right)$$\n",
    "\n",
    "### Second Hidden Layer\n",
    "\n",
    "\\begin{equation}\n",
    " \\begin{aligned}\n",
    "a^{(2)}_1 = w^{(2)}_{10} + w^{(2)}_{11}u^{(1)}_1 + w^{(2)}_{12}u^{(1)}_2 + w^{(2)}_{13}u^{(1)}_3 \\\\\n",
    "a^{(2)}_2 = w^{(2)}_{20} + w^{(2)}_{21}u^{(1)}_1 + w^{(2)}_{22}u^{(1)}_2 + w^{(2)}_{23}u^{(1)}_3 \\\\ \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "<br>\n",
    "\n",
    "**Then the sigmoid is applied**:\n",
    "\n",
    "\\begin{equation}\n",
    " \\begin{aligned}\n",
    "u^{(2)}_1 = \\sigma(a^{(2)}_1) \\\\\n",
    "u^{(2)}_2 = \\sigma(a^{(2)}_2) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "### Output Layer\n",
    "\n",
    "#### If the output is categorical:\n",
    "\n",
    "For example with four classes ($M=4$): $Y$={$y_1, y_2, y_3, y_4$}, we have the affine and then the sigmoid is lastly applied: \n",
    "\n",
    "\\begin{equation}\n",
    " \\begin{aligned}\n",
    "a^{(3)}_1 = w^{(3)}_{10} + w^{(3)}_{11}u^{(2)}_1 + w^{(3)}_{12}u^{(2)}_2 \\\\\n",
    "a^{(3)}_2 = w^{(3)}_{20} + w^{(3)}_{21}u^{(2)}_1 + w^{(3)}_{22}u^{(2)}_2 \\\\ \n",
    "a^{(3)}_3 = w^{(3)}_{30} + w^{(3)}_{31}u^{(2)}_1 + w^{(3)}_{32}u^{(2)}_2 \\\\\n",
    "a^{(3)}_4 = w^{(3)}_{40} + w^{(3)}_{41}u^{(2)}_1 + w^{(3)}_{42}u^{(2)}_2 \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "<br>\n",
    "\\begin{equation}\n",
    " \\begin{aligned}\n",
    "y_1 = \\sigma(a^{(3)}_1) \\\\\n",
    "y_2 = \\sigma(a^{(3)}_2) \\\\\n",
    "y_3 = \\sigma(a^{(3)}_3) \\\\\n",
    "y_3 = \\sigma(a^{(3)}_4) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$\\sigma$ will be softmax in the case of multiple classes and sigmoid for binary.\n",
    "<BR>\n",
    "    \n",
    "#### If the output is a number (regression):\n",
    "\n",
    "We have a single y as output:\n",
    "\n",
    "\\begin{equation}\n",
    " \\begin{aligned}\n",
    "y = w^{(3)}_{10}+ w^{(3)}_{11}u^{(2)}_1 + w^{(3)}_{12}u^{(2)}_2 + w^{(3)}_{13}u^{(2)}_3 \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication and constant addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 0], [0, 1], [2, 3]])\n",
    "b = np.array([[4, 1, 1], [2, 2, 1]])\n",
    "print(np.matrix(a))\n",
    "print('\\n------\\n')\n",
    "print(np.matrix(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both Tensorflow and numpy take care of transposing.\n",
    "c = tf.matmul(a, b) # the tensorflow way\n",
    "print(c)\n",
    "print('\\n------\\n')\n",
    "d = np.dot(a, b) # the numpy way\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we add the constant in the matrix\n",
    "a = [[1, 0], [0, 1]]\n",
    "ones = np.ones((len(a),1))   \n",
    "a = np.append(a, ones, axis=1)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 1: Plot the sigmoid function.</b></div>\n",
    "\n",
    "Define the `sigmoid` and the `tanh`. For `tanh` you may use `np.tanh` and for the `sigmoid` use the general equation:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma = \\dfrac{1}{1+e^{-2(x-c)/a}} \\qquad\\text{(1.1)}\n",
    "\\textrm{}\n",
    "\\end{align}\n",
    "\n",
    "Generate a list of 500 $x$ points from -5 to 5 and plot both functions. What do you observe? What do variables $c$ and $a$ do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sigmoid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 2: Approximate a Gaussian function using a node and manually adjusting the weights. Start with one layer with one node and move to two nodes.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to approximate (learn) a function $f\\left(x\\right)$ given some input $x$.  For demonstration purposes, the function we will try to learn is a Gaussian function: \n",
    "\\begin{align}\n",
    "f\\left(x\\right) = e^{-x^{2}}\n",
    "\\textrm{}\n",
    "\\end{align}\n",
    "\n",
    "Even though we represent the input $x$ as a vector on the computer, you should think of it as a single input.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Start by plotting the above function using the $x$ dataset you created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5.0, 5.0, 500) # input points\n",
    "def gaussian(x):\n",
    "    return np.exp(-x*x) \n",
    "\n",
    "f = gaussian(x)\n",
    "plt.plot(x, f, label='gaussian')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Now, let's code the single node as per the image above. \n",
    "\n",
    "Write a function named `affine` that does the transformation. The definition is provided below. Then create a simpler sigmoid with just one variable. We choose a **sigmoid** activation function and specifically the **logistic** function.  Sigmoids are a family of functions and the logistic function is just one member in that family. $$\\sigma\\left(z\\right) = \\dfrac{1}{1 + e^{-z}}.$$ <br> \n",
    "\n",
    "Define both functions in code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine(x, w, b):\n",
    "    \"\"\"Return affine transformation of x\n",
    "    \n",
    "    INPUTS\n",
    "    ======\n",
    "    x: A numpy array of points in x\n",
    "    w: An array representing the weight of the perceptron\n",
    "    b: An array representing the biases of the perceptron\n",
    "    \n",
    "    RETURN\n",
    "    ======\n",
    "    z: A numpy array of points after the affine transformation\n",
    "       z = wx + b\n",
    "    \"\"\"\n",
    "    \n",
    "    # Code goes here\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/affine-sigmoid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we plot the activation function and the true function. What do you think will happen if you change $w$ and $b$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [-5.0, 0.1, 5.0] # Create a list of weights\n",
    "b = [0.0, -1.0, 1.0] # Create a list of biases\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(9,5))\n",
    "SIZE = 16\n",
    "\n",
    "# plot our true function, the gaussian\n",
    "ax.plot(x, f, lw=4, ls='-.', label='True function')\n",
    "\n",
    "# plot 3 \"networks\"\n",
    "for wi, bi in zip(w, b):\n",
    "    h = sigmoid(affine(x, wi, bi))\n",
    "    ax.plot(x, h, lw=4, label=r'$w = {0}$, $b = {1}$'.format(wi,bi))\n",
    "    \n",
    "ax.set_title('Single neuron network', fontsize=SIZE)\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=SIZE) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=SIZE)\n",
    "\n",
    "ax.tick_params(labelsize=SIZE) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=SIZE, loc='best'); # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't do an exhaustive search of the weights and biases, but it sure looks like this single perceptron is never going to match the actual function.  Again, we shouldn't be suprised about this.  The output layer of the network is simple the logistic function, which can only have so much flexibility.\n",
    "\n",
    "Let's try to make our network more flexible by using **more nodes**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Perceptrons in a Single Layer\n",
    "It appears that a single neuron is somewhat limited in what it can accomplish.  What if we expand the number of nodes/neurons in our network?  We have two obvious choices here.  One option is to add depth to the network by putting layers next to each other.  The other option is to stack neurons on top of each other in the same layer.  Now the network has some width, but is still only one layer deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5.0, 5.0, 500) # input points\n",
    "f = np.exp(-x*x) # data\n",
    "\n",
    "w = np.array([3.5, -3.5])\n",
    "b = np.array([3.5, 3.5])\n",
    "\n",
    "# Affine transformations\n",
    "z1 = w[0] * x + b[0]\n",
    "z2 = w[1] * x + b[1]\n",
    "\n",
    "# Node outputs\n",
    "h1 = sigmoid(z1)\n",
    "h2 = sigmoid(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot things and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(9,5))\n",
    "\n",
    "ax.plot(x, f, lw=4, ls = '-.', label='True function')\n",
    "ax.plot(x, h1, lw=4, label='First neuron')\n",
    "ax.plot(x, h2, lw=4, label='Second neuron')\n",
    "\n",
    "# Set title\n",
    "ax.set_title('Comparison of Neuron Outputs', fontsize=SIZE)\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=SIZE) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=SIZE)\n",
    "\n",
    "ax.tick_params(labelsize=SIZE) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=SIZE, loc='best'); # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we expected.  Some sigmoids.  Of course, to get the network prediction we must combine these two sigmoid curves somehow.  First we'll just add $h_{1}$ and $h_{2}$ without any weights to see what happens.\n",
    "\n",
    "#### Note\n",
    "We are **not** doing classification here.  We are trying to predict an actual function.  The sigmoid activation is convenient when doing classification because you need to go from $0$ to $1$.  However, when learning a function, we don't have as good of a reason to choose a sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network output\n",
    "wout = np.ones(2) # Set the output weights to unity to begin\n",
    "bout = -1 # bias\n",
    "yout = wout[0] * h1 + wout[1] * h2 + bout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(9,5))\n",
    "\n",
    "ax.plot(x, f, ls='-.', lw=4, label=r'True function')\n",
    "ax.plot(x, yout, lw=4, label=r'$y_{out} = h_{1} + h_{2}$')\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=SIZE) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=SIZE)\n",
    "\n",
    "ax.tick_params(labelsize=SIZE) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=SIZE, loc='best') # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very cool!  The two nodes interact with each other to produce a pretty complicated-looking function.  It still doesn't match the true function, but now we have some hope.  In fact, it's starting to look a little bit like a Gaussian!\n",
    "\n",
    "We can do better.  There are three obvious options at this point:\n",
    "1. Change the number of nodes\n",
    "2. Change the activation functions\n",
    "3. Change the weights\n",
    "\n",
    "#### We will leave this simple example for some other time! Let's move on to fashion items!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tensors, Fashion, and Reese Witherspoon\n",
    "\n",
    "We can think of tensors as multidimensional arrays of real numerical values; their job is to generalize matrices to multiple dimensions. While tensors first emerged in the 20th century, they have since been applied to numerous other disciplines, including machine learning. Tensor decomposition/factorization can solve, among other, problems in unsupervised learning settings, temporal and multirelational data. For those of you that will get to handle images for Convolutional Neural Networks, it's a good idea to have the understanding of tensors of rank 3.\n",
    "\n",
    "We will use the following naming conventions:\n",
    "\n",
    "- scalar = just a number = rank 0 tensor  ($a$ ‚àà $F$,)\n",
    "<BR><BR>\n",
    "- vector = 1D array = rank 1 tensor ( $x = (\\;x_1,...,x_i\\;)‚ä§$ ‚àà $F^n$ )\n",
    "<BR><BR>\n",
    "- matrix = 2D array = rank 2 tensor ( $\\textbf{X} = [a_{ij}] ‚àà F^{m√ón}$ )\n",
    "<BR><BR>\n",
    "- 3D array = rank 3 tensor ( $\\mathscr{X} =[t_{i,j,k}]‚ààF^{m√ón√ól}$ )\n",
    "<BR><BR>\n",
    "- $\\mathscr{N}$D array = rank $\\mathscr{N}$ tensor ( $\\mathscr{T} =[t_{i1},...,t_{i\\mathscr{N}}]‚ààF^{n_1√ó...√ón_\\mathscr{N}}$ ) <-- Things start to get complicated here...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor indexing\n",
    "We can create subarrays by fixing some of the given tensor‚Äôs indices. We can create a vector by fixing all but one index. A 2D matrix is created when fixing all but two indices. For example, for a third order tensor the vectors are\n",
    "<br><BR>\n",
    "$\\mathscr{X}[:,j,k]$ = $\\mathscr{X}[j,k]$ (column), <br>\n",
    "$\\mathscr{X}[i,:,k]$ = $\\mathscr{X}[i,k]$ (row), and <BR>\n",
    "$\\mathscr{X}[i,j,:]$ = $\\mathscr{X}[i,j]$ (tube) <BR>\n",
    " \n",
    "#### Tensor multiplication\n",
    "We can multiply one matrix with another as long as the sizes are compatible ((n √ó m) √ó (m √ó p) = n √ó p), and also multiply an entire matrix by a constant. Numpy `numpy.dot` performs a matrix multiplication which is straightforward when we have 2D or 1D arrays. But what about > 3D arrays? The function will choose according to the matching dimentions but if we want to choose we should use `tensordot`, but, again, we **do not need tensordot** for this class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reese Witherspoon\n",
    "\n",
    "This image is from the dataset [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/person/Reese_Witherspoon.html) used for machine learning training. Images are 24-bit RGB images (height, width, channels) with 8 bits for each of R, G, B channel. Explore and print the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlopen(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(FILE)\n",
    "image_bytes = io.BytesIO(response.content)\n",
    "\n",
    "img = np.asarray(Image.open(image_bytes))\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image is 150x150 pixels with 3 color channels\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing tensors: slice along each axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to show each color channel\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10,10))\n",
    "for i, subplot in zip(range(3), axes):\n",
    "    temp = np.zeros(img.shape, dtype='uint8')\n",
    "    temp[:,:,i] = img[:,:,i]\n",
    "    subplot.imshow(temp)\n",
    "    subplot.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplying Images with a scalar (just for fun, does not really help us in any way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = img\n",
    "temp = temp * 2\n",
    "plt.imshow(temp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on image manipulation by `matplotlib` see: [matplotlib-images](https://matplotlib.org/3.1.1/tutorials/introductory/images.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of an Artificial Neural Network\n",
    "\n",
    "In Part 1 we hand-made a neural network by writing some simple python functions.  We focused on a regression problem where we tried to learn a function. We practiced using the logistic activation function in a network with multiple nodes, but a single or two hidden layers.  Some of the key observations were:\n",
    "* Increasing the number of nodes allows us to represent more complicated functions  \n",
    "* The weights and biases have a very big impact on the solution\n",
    "* Finding the \"correct\" weights and biases is really hard to do manually\n",
    "* There must be a better method for determining the weights and biases automatically\n",
    "\n",
    "We also didn't assess the effects of different activation functions or different network depths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![](https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/keras.png)\n",
    "\n",
    "https://www.tensorflow.org/guide/keras\n",
    "\n",
    "`tf.keras` is TensorFlow's high-level API for building and training deep learning models. It's used for fast prototyping, state-of-the-art research, and production. `Keras` is a library created by Fran√ßois Chollet. After Google released Tensorflow 2.0, the creators of `keras` recommend that \"Keras users who use multi-backend Keras with the TensorFlow backend switch to `tf.keras` in TensorFlow 2.0. `tf.keras` is better maintained and has better integration with TensorFlow features\".\n",
    "\n",
    "#### IMPORTANT:  In `Keras` everything starts with a Tensor of N samples as input and ends with a Tensor of N samples as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 3 parts of an ANN\n",
    "\n",
    "- **Part 1: the input layer** (our dataset)\n",
    "- **Part 2: the internal architecture or hidden layers** (the number of layers, the activation functions, the learnable parameters and other hyperparameters)\n",
    "- **Part 3: the output layer** (what we want from the network)\n",
    "\n",
    "In the rest of the lab we will practice with end-to-end neural network training\n",
    "\n",
    "1. Load the data \n",
    "2. Define the layers of the model.\n",
    "3. Compile the model.\n",
    "4. Fit the model to the train set (also using a validation set).\n",
    "5. Evaluate the model on the test set.\n",
    "6. Plot metrics such as accuracy.\n",
    "7. Predict on random images from test set.\n",
    "8. Predict on  a random image from the web!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST \n",
    "\n",
    "![](https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/drosophila.png)\n",
    "\n",
    "MNIST, the set of handwritten digits is considered the Drosophila (fruit flies) of Machine Learning. It has been overused, though, so we will try a slight modification to it.\n",
    "\n",
    "**Fashion-MNIST** is a dataset of clothing article images (created by [Zalando](https://github.com/zalandoresearch/fashion-mnist)), consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a **28 x 28** grayscale image, associated with a label from **10 classes**. The creators intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. Each pixel is 8 bits so its value ranges from 0 to 255.\n",
    "\n",
    "Let's load and look at it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get the data from keras\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "# load the data splitted in train and test! how nice!\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# normalize the data by dividing with pixel intensity\n",
    "# (each pixel is 8 bits so its value ranges from 0 to 255)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# classes are named 0-9 so define names for plotting clarity\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[3], cmap=plt.cm.binary);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  \n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    \n",
    "    tf.keras.layers.Dense(154, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    #to_file='model.png', # if you want to save the image\n",
    "    show_shapes=True, # True for more details than you need\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Everything you wanted to know about a Keras Model and were afraid to ask](https://www.tensorflow.org/api_docs/python/tf/keras/Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fit the model to the train set (also using a validation set)\n",
    "\n",
    "This is the part that takes the longest.\n",
    "\n",
    "-----------------------------------------------------------\n",
    "**ep¬∑och** <BR>\n",
    "noun: epoch; plural noun: epochs. A period of time in history or a person's life, typically one marked by notable events or particular characteristics. Examples: \"the Victorian epoch\", \"my Neural Netwok's epochs\". <BR>\n",
    "    \n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# the core of the network training\n",
    "history = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    validation_split=0.33, \n",
    "                    epochs=50, \n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model\n",
    "\n",
    "You can save the model so you do not have `.fit` everytime you reset the kernel in the notebook. Network training is expensive!\n",
    "\n",
    "For more details on this see [https://www.tensorflow.org/guide/keras/save_and_serialize](https://www.tensorflow.org/guide/keras/save_and_serialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model so you do not have to run the code everytime\n",
    "model.save('fashion_model.h5')\n",
    "\n",
    "# Recreate the exact same model purely from the file\n",
    "#model = tf.keras.models.load_model('fashion_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test accuracy={test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. We learn a lot by studying History! Plot metrics such as accuracy. \n",
    "\n",
    "You can learn a lot about neural networks by observing how they perform while training. You can issue `callbacks` in `keras`. The networks's performance is stored in a `keras` callback aptly named `history` which can be plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy and loss for the test set\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(history.history['accuracy'])\n",
    "ax[0].plot(history.history['val_accuracy'])\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend(['train', 'val'], loc='best')\n",
    "\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[1].plot(history.history['val_loss'])\n",
    "ax[1].set_title('Model loss')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend(['train', 'val'], loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Now let's use the Network for what it was meant to do: Predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predictions[0]), class_names[np.argmax(predictions[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our network predicted right! Is the first item what was predicted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_test[0], cmap=plt.cm.binary)\n",
    "plt.xlabel(class_names[y_test[0]])\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correct!!** Now let's see how confident our model is by plotting the probability values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code source: https://www.tensorflow.org/tutorials/keras/classification\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array, true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 406\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i, predictions[i], y_test, x_test)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i, predictions[i],  y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Predicting in the real world\n",
    "\n",
    "Let's see if our network can generalize beyond the MNIST fashion dataset. Let's give it an random googled image of a boot. Does it have to be a clothing item resembling the MNIST fashion dataset? Can it be a puppy?\n",
    "\n",
    "Download an image from the internet and resize it to 28x28. Here is the one I choose:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/random_boot.png\" alt=\"random boot\" width=\"150\" height=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the one I chose from the web. I resized it to 28 x 28 and one channel. \n",
    "response = requests.get('https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/random_boot.png')\n",
    "image_bytes = io.BytesIO(response.content)\n",
    "\n",
    "random_boot = np.array(Image.open(image_bytes))\n",
    "random_boot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into one channel\n",
    "random_boot = random_boot[:,:,0]\n",
    "random_boot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(random_boot, cmap=plt.cm.binary)\n",
    "plt.xlabel('random boot from web')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras` models are optimized to make predictions on a batch, or collection, of examples at once. Accordingly, even though you're using a single image, you need to add it to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the image to a batch where it's the only member.\n",
    "random_batch = (np.expand_dims(random_boot,0))\n",
    "print(random_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_single = model.predict(random_batch)\n",
    "print(predictions_single[0])\n",
    "print(np.argmax(predictions_single[0]), class_names[np.argmax(predictions_single[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Were we successful???\n",
    "\n",
    "Let's now try a different one, a dress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/dress.jpeg')\n",
    "image_bytes = io.BytesIO(response.content)\n",
    "\n",
    "dress = np.array(Image.open(image_bytes))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(dress, cmap=plt.cm.binary)\n",
    "plt.xlabel('Random dress from web')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into one channel\n",
    "dress = dress[:,:,0]\n",
    "dress.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dresses = (np.expand_dims(dress,0))\n",
    "print(dresses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_single = model.predict(dresses)\n",
    "print(predictions_single[0])\n",
    "print(np.argmax(predictions_single[0]), class_names[np.argmax(predictions_single[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did not do so well this time... Well, our model accuracy is not that good anyway. \n",
    "\n",
    "Check out Convolutional Neural Networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
