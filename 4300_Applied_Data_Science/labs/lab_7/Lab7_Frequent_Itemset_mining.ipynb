{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='heading'>\n",
    "    <div style='float:left;'><h1>CPSC 4300/6300: Applied Data Science</h1></div>\n",
    "    <img style=\"float: right; padding-right: 10px; width: 65px\" src=\"https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/images/clemson_paw.png\"> </div>\n",
    "\n",
    "\n",
    "## Week 7: Frequent Itemset Mining & Rule Association\n",
    "\n",
    "**Clemson University**<br>\n",
    "**Spring 2022**<br>\n",
    "**Instructor(s):** Nina Hubig <br>\n",
    "**Author(s):** Chris Kalahiki\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RUN THIS CELL TO GET THE RIGHT FORMATTING \"\"\"\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "css_file = 'https://raw.githubusercontent.com/bsethwalker/clemson-cs4300/main/css/cpsc6300.css'\n",
    "styles = requests.get(css_file).text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning: In addition to the packages that we have used up to this point, we will also be using the mlxtend package for some frequent itemset mining functionality. Be sure to install this package before continuing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "- Understand the Apriori algorithm\n",
    "- Make use of the mlxtend package to mine frequent itemset sets\n",
    "- Create a couple new visualizations that show the results of frequent itemset mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Aprior Algorithm \n",
    "\n",
    "* Inventation \n",
    "> This algorithm, introduced by R Agrawal and R Srikant in 1994 has great significance in data mining. We shall see the importance of the apriori algorithm in data mining in this article\n",
    "* Brief Introduction with Examples \n",
    "This small story will help you understand the concept better. You must have noticed that the local vegetable seller always bundles onions and potatoes together. He even offers a discount to people who buy these bundles.\n",
    "\n",
    "**Why does he do so?** \n",
    "\n",
    "He realises that people who buy potatoes also buy onions. Therefore, by bunching them together, he makes it easy for the customers. At the same time, he also increases his sales performance. It also allows him to offer discounts.Similarly, you go to a supermarket, and you will find bread, butter, and jam bundled together. It is evident that the idea is to make it comfortable for the customer to buy these three food items in the same place.\n",
    "\n",
    "\n",
    "The Walmart beer diaper parable is another example of this phenomenon. People who buy diapers tend to buy beer as well. The logic is that raising kids is a stressful job. People take beer to relieve stress. Walmart saw a spurt in the sale of both diapers and beer.\n",
    "\n",
    "* What is the Apriori Algorithm? \n",
    "\n",
    "**Apriori algorithm, a classic algorithm, is useful in mining frequent itemsets and relevant association rules. Usually, you operate this algorithm on a database containing a large number of transactions. One such example is the items customers buy at a supermarket.**\n",
    "\n",
    "It helps the customers buy their items with ease, and enhances the sales performance of the departmental store.\n",
    "\n",
    "This algorithm has utility in the field of healthcare as it can help in detecting adverse drug reactions (ADR) by producing association rules to indicate the combination of medications and patient characteristics that could lead to ADRs\n",
    "\n",
    "* Apriori Algorithm – An Odd Name\n",
    "It has got this odd name because it uses ‘prior’ knowledge of frequent itemset properties. The credit for introducing this algorithm goes to Rakesh Agrawal and Ramakrishnan Srikant in 1994. We shall now explore the apriori algorithm implementation in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads and displays our data\n",
    "data = pd.read_csv(\"data/groceries - groceries.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=20,7\n",
    "sns.countplot(data=data, x=data['Item 1'], order=data['Item 1'].value_counts().head(20).index)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Product')\n",
    "plt.title('Top 20 frequently bought products')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a list of transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for i in range(0, len(data)):\n",
    "    records.append([str(data.values[i,j]) for j in range(1, data.values[i, 0]+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the no. of items wrt each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [len(record) for record in records]\n",
    "print(f'50 percent of the transactions are having items below or equal to {np.quantile(counts, .5)} only')\n",
    "print(f'particular transaction having a maximum of {np.quantile(counts, 1)} items in it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a one-hot encoder to use going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "onehot = te.fit_transform(records)\n",
    "onehot = pd.DataFrame(onehot, columns = te.columns_)\n",
    "\n",
    "print(f'Shape of encoded data: {onehot.shape[0]} rows and {onehot.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to complexity we are reducing the number of items by selecting particular items alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = onehot.loc[:, ['bottled beer', 'bottled water', 'brandy', 'brown bread', 'butter', 'syrup',\n",
    "                    'sweet spreads', 'beverages', 'berries', 'beef', 'bathroom cleaner', 'baking powder', 'bags',\n",
    "                    'baby food', 'baby cosmetics', 'Instant food products', 'tea', 'toilet cleaner', 'vinegar', 'waffles', 'whisky',\n",
    "                    'white bread', 'white wine', 'yogurt', 'zwieback', 'whole milk', 'whipped/sour cream', 'abrasive cleaner']]\n",
    "\n",
    "print(f'Shape of encoded data: {onehot.shape[0]} rows and {onehot.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the Apriori Algorithm in Data Mining work?\n",
    "\n",
    "* Consider a supermarket scenario where the itemset is I = {Onion, Burger, Potato, Milk, Beer}. The database consists of six transactions where 1 represents the presence of the item and 0 the absence.\n",
    "\n",
    "![](https://www.digitalvidya.com/wp-content/uploads/2018/11/01.jpg)\n",
    "\n",
    "#### Step -01 \n",
    "Create a frequency table of all the items that occur in all the transactions. Now, prune the frequency table to include only those items having a threshold support level over 50%. We arrive at this frequency table.\n",
    "\n",
    "* All subsets of a frequent itemset should be frequent.\n",
    "* In the same way, the subsets of an infrequent itemset should be infrequent.\n",
    "* Set a threshold support level. In our case, we shall fix it at 50%\n",
    "\n",
    "![](https://www.digitalvidya.com/wp-content/uploads/2018/11/02.jpg)\n",
    "\n",
    "#### Step -02 \n",
    "\n",
    "* Make pairs of items such as OP, OB, OM, PB, PM, BM. This frequency table is what you arrive at.\n",
    "\n",
    "![](https://www.digitalvidya.com/wp-content/uploads/2018/11/03.jpg)\n",
    "\n",
    "#### Step -03\n",
    "Apply the same threshold support of 50% and consider the items that exceed 50% (in this case 3 and above).\n",
    "Thus, you are left with OP, OB, PB, and PM\n",
    "#### Step -04\n",
    "Step 4\n",
    "Look for a set of three items that the customers buy together. Thus we get this combination.\n",
    "* OP and OB gives OPB\n",
    "* PB and PM gives PBM\n",
    "#### Step -05\n",
    "\n",
    "![](https://www.digitalvidya.com/wp-content/uploads/2018/11/04.jpg)\n",
    "*  Determine the frequency of these two itemsets. You get this frequency table.If you apply the threshold assumption, you can deduce    that the set of three items frequently purchased by the customers is OPB.\n",
    "* We have taken a simple example to explain the apriori algorithm in data mining. In reality, you have hundreds and thousands of such combinations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the frequent itemset using apriori with minimum support of 0.01% and maximum item per transaction restricted to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frequent_itemsets=apriori(onehot, min_support=0.0001, use_colnames=True, max_len=3)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Detailed Theory \n",
    "\n",
    "**Three significant components comprise the apriori algorithm. They are as follows.**\n",
    "\n",
    "* Support\n",
    "* Confidence\n",
    "* Lift\n",
    "\n",
    "**This example will make things easy to understand. Let us suppose you have 2000 customer transactions in a supermarket. You have to find the Support, Confidence, and Lift for two items, say bread and jam. It is because people frequently bundle these two items together.**\n",
    "Out of the 2000 transactions, 200 contain jam whereas 300 contain bread. These 300 transactions include a 100 that includes bread as well as jam. Using this data, we shall find out the support, confidence, and lift.\n",
    "#### SUPPORT \n",
    "Support is the default popularity of any item. You calculate the Support as a quotient of the division of the number of transactions containing that item by the total number of transactions. Hence, in our example,\n",
    "\n",
    "* Support (Jam) = (Transactions involving jam) / (Total Transactions)\n",
    "\n",
    "                       *  = 200/2000 = 10%\n",
    "#### Confidence \n",
    "It is the likelihood that customer bought both bread and jam. Dividing the number of transactions that include both bread and jam by the total number of transactions will give the Confidence figure.\n",
    "\n",
    "* Confidence = (Transactions involving both bread and jam) / (Total Transactions involving jam)\n",
    "\n",
    "                    = 100/200 = 50%                      \n",
    "                       \n",
    "  * It implies that 50% of customers who bought jam bought bread as well\n",
    "  \n",
    "####  Lift\n",
    "**According to our example, Lift is the increase in the ratio of the sale of bread when you sell jam. The mathematical formula of Lift is as follows.**\n",
    "\n",
    "* Lift = (Confidence (Jam͢͢ – Bread)) / (Support (Jam))\n",
    "\n",
    "      = 50 / 10 = 5\n",
    "It says that the likelihood of a customer buying both jam and bread together is 5 times more than the chance of purchasing jam alone. If the Lift value is less than 1, it entails that the customers are unlikely to buy both the items together. Greater the value, the better is the combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will generate association rules with mlxtend's association_rules function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules=association_rules(frequent_itemsets)\n",
    "rules.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try pruning the generated rules using multiple filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_rules = rules[rules['antecedents'] == {'baby food'}].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise #1</b>: </div>\n",
    "\n",
    "Using the targeted_rules DataFrame above, filter the rules to retain only those rules that meet both of the following criteria:\n",
    "- Rules with a confidence greater than 0.85\n",
    "- Rules with a lift greater than 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise1-solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can take a look at the 'baby foods' relevant consequents. To do this, we will use a [`pivot table`](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html) using pandas. From there, we will use seaborn's [`heatmap`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15, 15)\n",
    "support_table = filtered_rules.pivot(index='consequents', columns='antecedents', values='lift')\n",
    "sns.heatmap(support_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules['antecedent'] = rules['antecedents'].apply(lambda antecedent: list(antecedent)[0])\n",
    "rules['consequent'] = rules['consequents'].apply(lambda consequent: list(consequent)[0])\n",
    "rules['rule'] = rules.index\n",
    "coords = rules[['antecedent','consequent','rule']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can generate a parallel coordinate plot to visualize the association rules.\n",
    "\n",
    "<div class=\"exercise\"><b>Exercise #2</b>: </div>\n",
    "\n",
    "Visualize the association rules using a [`parallel coordinate`](https://pandas.pydata.org/docs/reference/api/pandas.plotting.parallel_coordinates.html) plot. Because of the large number of rules, we will need to modify the [`legend`](https://matplotlib.org/3.5.1/api/_as_gen/matplotlib.pyplot.legend.html) of the plot. For the best results, I recommend using three columns for the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise2-solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apriori Algorithm – Pros\n",
    "* Easy to understand and implement\n",
    "* Can use on large itemsets\n",
    "#####  Apriori Algorithm – Cons\n",
    "* At times, you need a large number of candidate rules. It can become computationally expensive.\n",
    "* It is also an expensive method to calculate support because the calculation has to go through the entire database.\n",
    "##### Apriori Algorithm – Limitations\n",
    "* The process can sometimes be very tedious.\n",
    "* How to Improve the Efficiency of the Apriori Algorithm?\n",
    "\n",
    "#### Use the following methods to improve the efficiency of the apriori algorithm.\n",
    "\n",
    "* Transaction Reduction – A transaction not containing any frequent k-itemset becomes useless in subsequent scans.\n",
    "* Hash-based Itemset Counting – Exclude the k-itemset whose corresponding hashing bucket count is less than the threshold is an infrequent itemset.\n",
    "* There are other methods as well such as partitioning, sampling, and dynamic itemset counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57990f1ad2ea89c67ddae7f31d40c478205c5912da0fccfb7c5cfbb2b8bf17ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
